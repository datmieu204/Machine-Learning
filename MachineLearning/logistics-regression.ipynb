{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài toán phân lớp 2 lớp\n",
    "Cho đầu vào $x\\in\\mathbb R^d$, phân lớp $x$ vào một trong hai lớp $y = h(x) \\in \\{0,1\\}$ biết dữ liệu\n",
    "$$D=\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình Hồi quy Logistics\n",
    "\n",
    "$$\n",
    "Y|X=x \\sim Ber(y|\\sigma(f(x)))\n",
    "$$\n",
    "\n",
    "trong đó\n",
    "\n",
    "$$\n",
    "f(x) = w^T x + w_0\\cdot 1\n",
    "$$\n",
    "\n",
    "và hàm Sigmoid\n",
    "$$\\sigma(z) = \\frac 1 {1+e^{-z}} = \\frac{e^z}{e^z+e^0}=\\mathcal S(z, 0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phương trình mặt phẳng\n",
    "\n",
    "$$\n",
    "w^T x+ w_0 = 0\n",
    "$$\n",
    "\n",
    "Hỏi điểm $x\\in\\mathbb R^d$ cách mặt phẳng bao nhiêu, giả sử $\\|w\\|=1$\n",
    "\n",
    "$$\n",
    "d(x) = \\frac {|f(x)=w^Tx+w_0|} {\\|w\\|} = |f(x)| = |w^Tx+w_0|\n",
    "$$\n",
    "\n",
    "$$\n",
    "Ax+By+C = [A,B]^T [x,y] + C =0 \n",
    "$$\n",
    "\n",
    "Cho $(x, y)$ thì khoảng cách đến đường thẳng\n",
    "\n",
    "$$\n",
    "d = \\frac {|Ax+By+C|}{\\sqrt{A^2+B^2}}\n",
    "$$\n",
    "\n",
    "### Quyết định phân lớp\n",
    "\n",
    "$$\n",
    "h(x) = \\begin{cases}1 & \\textrm{nếu }f(x) \\geq 0\\\\ 0 & \\textrm{nếu } f(x) < 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "Tương ứng ta có\n",
    "\n",
    "$f(x) \\geq 0$ thì $P(Y=1|x) \\geq 0.5$\n",
    "\n",
    "$f(x) < 0$ thì $P(Y=1|x) < 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện Mô hình hồi quy Logistics\n",
    "\n",
    "Nguyên lý MLE\n",
    "\n",
    "1. Sự hợp lý $L(w, w_0)$\n",
    "\n",
    "$$\n",
    "P(y_1, y_2, \\ldots, y_n | x_1, x_2, \\ldots x_n) = \\prod_{i=1}^n P(y_i|x_i) = \\prod_{i=1}^n \\mu_i^{y_i} (1-\\mu_i)^{1-y_i}\n",
    "$$\n",
    "\n",
    "với $\\mu_i = \\sigma(f(x_i))$\n",
    "\n",
    "2. Lấy -log (negative log-likelihood - NLL)\n",
    "\n",
    "$$\n",
    "\\ell(w, w_0) = \\sum_{i=1}^n -y_i \\log \\mu_i - (1-y_i)\\log (1-\\mu_i)\n",
    "$$\n",
    "\n",
    "tên gọi hàm lỗi trên là ***entropy chéo nhị phân*** (binary cross entropy - BCE)\n",
    "\n",
    "3. Cực tiểu hóa NLL\n",
    "\n",
    "Khó để giải hệ phương trình\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\nabla_w \\ell(w, w_0) &= 0\\\\ \\nabla_{w_0} \\ell(w, w_0) &= 0\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Xuống đồi bằng đạo hàm (Gradient descent)\n",
    "\n",
    "$$\n",
    "\\begin{align*} w &\\leftarrow w-\\lambda\\nabla_w \\ell(w, w_0) \\\\ w_0 &\\leftarrow w_0 -\\lambda \\nabla_{w_0} \\ell(w, w_0) \\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(f(x))' = \\sigma(f(x)) (1-\\sigma(f(x))) f'(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*} \\nabla_w \\ell(w, w_0) &= \\sum_{i=1}^n \\frac{\\partial \\ell}{\\partial \\mu_i}\\frac{\\partial\\mu_i}{\\partial w}\\\\\n",
    "&=\\sum_{i=1}^n \\left(-\\frac {y_i} {\\mu_i}+\\frac {1-y_i} {1-\\mu_i}\\right) \\mu_i (1-\\mu_i)x_i \\in \\mathbb R^d\\\\\n",
    "&=\\sum_{i=1}^n (-y_i(1-\\mu_i) + (1-y_i)\\mu_i) x_i\\\\\n",
    "&=\\sum_{i=1}^n (\\mu_i-y_i) x_i\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "$\\mu_i - y_i$ sai lệch giữa xác suất tính được và xác suất mong muốn đối với dữ liệu thứ $i$.\n",
    "\n",
    "Đạo hàm = tổng các sai lệch, được đánh trọng số bởi dữ liệu $x_i$\n",
    "\n",
    "$$\n",
    "\\nabla_{w_0} \\ell(w, w_0) = \\sum_{i=1}^n \\mu_i-y_i\n",
    "$$\n",
    "\n",
    "### Thuật toán huấn luyện LR bằng GD\n",
    "\n",
    "$\\mathrm{TrainLogisticsRegressionGD}(D, \\lambda)$:\n",
    "\n",
    "1. Khởi tạo: $w = 0, w_0 = 0$\n",
    "2. Lặp $epoch = 1, 2, \\ldots$\n",
    "    1. Tính đạo hàm $\\nabla_{w} \\ell(w, w_0), \\nabla_{w_0} \\ell(w, w_0)$\n",
    "    2. Cập nhật tham số\n",
    "    \n",
    "    $$\n",
    "    \\begin{align*} w &\\leftarrow w-\\lambda\\nabla_w \\ell(w, w_0) \\\\ w_0 &\\leftarrow w_0 -\\lambda \\nabla_{w_0} \\ell(w, w_0) \\end{align*}\n",
    "    $$\n",
    "    \n",
    "3. Dừng khi:\n",
    "    1. epoch đủ lớn\n",
    "    2. đạo hàm đủ nhỏ $\\|\\nabla_{w} \\ell(w, w_0)\\|, \\nabla_{w_0} \\ell(w, w_0) \\rightarrow 0$\n",
    "    3. Khuyến cáo: giảm dần $\\lambda$\n",
    "\n",
    "$\\lambda$: ***hyper-parameter*** phải thử $1, 0.1, 0.01, 10^{-3}, 10^{-4}, \\ldots$\n",
    "\n",
    "Cách giảm:\n",
    "\n",
    "$\\lambda_t = \\lambda_0 / \\sqrt{t}$\n",
    "\n",
    "- Dùng toàn bộ dữ liệu —> batch optimization —> overfit nhanh hơn, tính lâu\n",
    "- Dùng một phần nhỏ mỗi lần lặp —> mini-batch optimization —> kích thước: siêu tham số\n",
    "- Dùng một điểm dữ liệu —> stochastic optimization —> khó bị cực trị địa phương, tính nhanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, learning_rate=0.001, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros((n_features, 1))\n",
    "        self.bias = 0\n",
    "\n",
    "        cost_list = []\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            z = np.dot(X, self.weights) + self.bias  # linear prediction\n",
    "            a = sigmoid(z)  # predictions\n",
    "\n",
    "            # Avoid log(0) by adding a small value to a and 1-a\n",
    "            loss = -(y * np.log(a + 1e-10)) - ((1 - y) * np.log(1 - a + 1e-10))\n",
    "            cost = (1 / n_samples) * np.sum(loss)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (a - y))\n",
    "            db = (1 / n_samples) * np.sum(a - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            cost_list.append(cost)\n",
    "            if i % (self.iterations // 10) == 0:\n",
    "                print(\"Cost after\", i, \"iterations:\", cost)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        class_pred = [0 if y <= 0.5 else 1 for y in y_pred]\n",
    "        return class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iterations: 0.6931471803599452\n",
      "Cost after 300 iterations: 0.3954158685829803\n",
      "Cost after 600 iterations: 0.30140839026680716\n",
      "Cost after 900 iterations: 0.2532057258239297\n",
      "Cost after 1200 iterations: 0.22286547117906932\n",
      "Cost after 1500 iterations: 0.20156680451304296\n",
      "Cost after 1800 iterations: 0.1855852543461823\n",
      "Cost after 2100 iterations: 0.17304709076586958\n",
      "Cost after 2400 iterations: 0.1628916048821655\n",
      "Cost after 2700 iterations: 0.15446543887612027\n",
      "Accuracy: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "clf = LogisticRegression(learning_rate=0.001, iterations=3000)\n",
    "clf.fit(X_train, y_train.reshape(-1, 1))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng mô hình\n",
    "\n",
    "### Độ đo (metrics)\n",
    "\n",
    "- accuracy: số lượng mẫu đoán đúng / tổng số mẫu\n",
    "\n",
    "$$\n",
    "acc = \\frac{\\sum_{i=1}^n \\mathbb I(y_i = h(x_i))}{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb I(p) = \\begin{cases}1&  \\textrm{nếu $p$ đúng}\\\\0 & \\textrm{nếu $p$ sai}\\end{cases}\n",
    "$$\n",
    "\n",
    "gọi là hàm chỉ báo (indicator function).\n",
    "\n",
    "- error: số lượng mẫu đoán sai trên tổng số mẫu\n",
    "\n",
    "$$\n",
    "err = \\frac{\\sum_{i=1}^n \\mathbb I(y_i \\neq h(x_i))}{n}\n",
    "$$\n",
    "\n",
    "- precision của lớp A: số lượng mẫu đoán đúng là A / số lượng mẫu đoán là A\n",
    "\n",
    "$$\n",
    "p = \\frac{\\sum_{i=1}^n \\mathbb I(y_i = h(x_i) = A)}{\\sum_{i=1}^n \\mathbb I(h(x_i) = A)}\n",
    "$$\n",
    "\n",
    "- recall của lớp A:  số lượng mẫu đoán đúng là A / số lượng mẫu là A\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^n \\mathbb I(y_i = h(x_i) = A)}{\\sum_{i=1}^n \\mathbb I(y_i = A)}\n",
    "$$\n",
    "\n",
    "- f1 của lớp A\n",
    "$$\n",
    "\\mathrm{F1} = \\frac 2 {\\frac 1 p + \\frac 1 r}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mieu1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
